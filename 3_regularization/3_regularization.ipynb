{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "kR-4eNdK6lYS"
   },
   "source": [
    "Deep Learning\n",
    "=============\n",
    "\n",
    "Assignment 3\n",
    "------------\n",
    "\n",
    "Previously in `2_fullyconnected.ipynb`, you trained a logistic regression and a neural network model.\n",
    "\n",
    "The goal of this assignment is to explore regularization techniques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "JLpLa8Jt7Vu4"
   },
   "outputs": [],
   "source": [
    "# These are all the modules we'll be using later. Make sure you can import them\n",
    "# before proceeding further.\n",
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from six.moves import cPickle as pickle\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "\n",
    "import timeit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1HrCK6e17WzV"
   },
   "source": [
    "First reload the data we generated in `1_notmnist.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11777,
     "status": "ok",
     "timestamp": 1449849322348,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "y3-cj1bpmuxc",
    "outputId": "e03576f1-ebbe-4838-c388-f1777bcc9873",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 28, 28) (200000,)\n",
      "Validation set (10000, 28, 28) (10000,)\n",
      "Test set (10000, 28, 28) (10000,)\n"
     ]
    }
   ],
   "source": [
    "pickle_file = '../1_notmnist/notMNIST.pickle'\n",
    "\n",
    "with open(pickle_file, 'rb') as f:\n",
    "  save = pickle.load(f)\n",
    "  train_dataset = save['train_dataset']\n",
    "  train_labels = save['train_labels']\n",
    "  valid_dataset = save['valid_dataset']\n",
    "  valid_labels = save['valid_labels']\n",
    "  test_dataset = save['test_dataset']\n",
    "  test_labels = save['test_labels']\n",
    "  del save  # hint to help gc free up memory\n",
    "  print('Training set', train_dataset.shape, train_labels.shape)\n",
    "  print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "  print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "L7aHrm6nGDMB"
   },
   "source": [
    "Reformat into a shape that's more adapted to the models we're going to train:\n",
    "- data as a flat matrix,\n",
    "- labels as float 1-hot encodings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "output_extras": [
      {
       "item_id": 1
      }
     ]
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11728,
     "status": "ok",
     "timestamp": 1449849322356,
     "user": {
      "color": "",
      "displayName": "",
      "isAnonymous": false,
      "isMe": true,
      "permissionId": "",
      "photoUrl": "",
      "sessionId": "0",
      "userId": ""
     },
     "user_tz": 480
    },
    "id": "IRSyYiIIGIzS",
    "outputId": "3f8996ee-3574-4f44-c953-5c8a04636582"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training set (200000, 784) (200000, 10)\n",
      "Validation set (10000, 784) (10000, 10)\n",
      "Test set (10000, 784) (10000, 10)\n"
     ]
    }
   ],
   "source": [
    "image_size = 28\n",
    "num_labels = 10\n",
    "\n",
    "def reformat(dataset, labels):\n",
    "  dataset = dataset.reshape((-1, image_size * image_size)).astype(np.float32)\n",
    "  # Map 1 to [0.0, 1.0, 0.0 ...], 2 to [0.0, 0.0, 1.0 ...]\n",
    "  labels = (np.arange(num_labels) == labels[:,None]).astype(np.float32)\n",
    "  return dataset, labels\n",
    "train_dataset, train_labels = reformat(train_dataset, train_labels)\n",
    "valid_dataset, valid_labels = reformat(valid_dataset, valid_labels)\n",
    "test_dataset, test_labels = reformat(test_dataset, test_labels)\n",
    "print('Training set', train_dataset.shape, train_labels.shape)\n",
    "print('Validation set', valid_dataset.shape, valid_labels.shape)\n",
    "print('Test set', test_dataset.shape, test_labels.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "cellView": "both",
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "RajPLaL_ZW6w"
   },
   "outputs": [],
   "source": [
    "def accuracy(predictions, labels):\n",
    "  return (100.0 * np.sum(np.argmax(predictions, 1) == np.argmax(labels, 1))\n",
    "          / predictions.shape[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sgLbUAQ1CW-1"
   },
   "source": [
    "---\n",
    "Problem 1\n",
    "---------\n",
    "\n",
    "Introduce and tune L2 regularization for both logistic and neural network models. Remember that L2 amounts to adding a penalty on the norm of the weights to the loss. In TensorFlow, you can compute the L2 loss for a tensor `t` using `nn.l2_loss(t)`. The right amount of regularization should improve your validation / test accuracy.\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic regression\n",
    "First the graph for logistic regression with L2 regularization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "L2_reg = 0.001\n",
    "\n",
    "graph = tf.Graph()\n",
    "with graph.as_default():\n",
    "\n",
    "  # Input data. For the training data, we use a placeholder that will be fed\n",
    "  # at run time with a training minibatch.\n",
    "  tf_train_dataset = tf.placeholder(tf.float32,\n",
    "                                    shape=(batch_size, image_size * image_size))\n",
    "  tf_train_labels = tf.placeholder(tf.float32, shape=(batch_size, num_labels))\n",
    "  tf_valid_dataset = tf.constant(valid_dataset)\n",
    "  tf_test_dataset = tf.constant(test_dataset)\n",
    "  beta_regul = tf.placeholder(tf.float32)\n",
    "  \n",
    "  # Variables.\n",
    "  weights = tf.Variable(\n",
    "    tf.truncated_normal([image_size * image_size, num_labels]))\n",
    "  biases = tf.Variable(tf.zeros([num_labels]))\n",
    "  \n",
    "  # Training computation.\n",
    "  logits = tf.matmul(tf_train_dataset, weights) + biases\n",
    "  loss = tf.reduce_mean(\n",
    "    tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=tf_train_labels)) + beta_regul * tf.nn.l2_loss(weights)\n",
    "  \n",
    "  # Optimizer.\n",
    "  optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "  \n",
    "  # Predictions for the training, validation, and test data.\n",
    "  train_prediction = tf.nn.softmax(logits)\n",
    "  valid_prediction = tf.nn.softmax(\n",
    "    tf.matmul(tf_valid_dataset, weights) + biases)\n",
    "  test_prediction = tf.nn.softmax(tf.matmul(tf_test_dataset, weights) + biases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/matija/.local/lib/python3.5/site-packages/tensorflow/python/util/tf_should_use.py:118: initialize_all_variables (from tensorflow.python.ops.variables) is deprecated and will be removed after 2017-03-02.\n",
      "Instructions for updating:\n",
      "Use `tf.global_variables_initializer` instead.\n",
      "Initialized\n",
      "Minibatch loss at step 0: 18.799061\n",
      "Minibatch accuracy: 6.2%\n",
      "Validation accuracy: 11.3%\n",
      "Minibatch loss at step 500: 2.526260\n",
      "Minibatch accuracy: 78.9%\n",
      "Validation accuracy: 77.1%\n",
      "Minibatch loss at step 1000: 2.024815\n",
      "Minibatch accuracy: 74.2%\n",
      "Validation accuracy: 79.3%\n",
      "Minibatch loss at step 1500: 1.113453\n",
      "Minibatch accuracy: 79.7%\n",
      "Validation accuracy: 81.0%\n",
      "Minibatch loss at step 2000: 0.825109\n",
      "Minibatch accuracy: 85.9%\n",
      "Validation accuracy: 81.3%\n",
      "Minibatch loss at step 2500: 0.721640\n",
      "Minibatch accuracy: 82.0%\n",
      "Validation accuracy: 82.0%\n",
      "Minibatch loss at step 3000: 0.705919\n",
      "Minibatch accuracy: 84.4%\n",
      "Validation accuracy: 81.8%\n",
      "Test accuracy: 88.4%\n"
     ]
    }
   ],
   "source": [
    "num_steps = 3001\n",
    "\n",
    "with tf.Session(graph=graph) as session:\n",
    "  tf.initialize_all_variables().run()\n",
    "  print(\"Initialized\")\n",
    "  for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "    offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "    batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "    batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "    feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : 1e-3}\n",
    "    _, l, predictions = session.run(\n",
    "      [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    if (step % 500 == 0):\n",
    "      print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "      print(\"Minibatch accuracy: %.1f%%\" % accuracy(predictions, batch_labels))\n",
    "      print(\"Validation accuracy: %.1f%%\" % accuracy(\n",
    "        valid_prediction.eval(), valid_labels))\n",
    "  print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "L2 regularization - BETA parameter, iterate and change beta parameter."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "regul_val = [pow(10, i) for i in np.arange(-4, -2, 0.1)]\n",
    "accuracy_val = []\n",
    "\n",
    "for regul in regul_val:\n",
    "  with tf.Session(graph=graph) as session:\n",
    "    tf.initialize_all_variables().run()\n",
    "    for step in range(num_steps):\n",
    "    # Pick an offset within the training data, which has been randomized.\n",
    "    # Note: we could use better randomization across epochs.\n",
    "      offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "    # Generate a minibatch.\n",
    "      batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "      batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "    # Prepare a dictionary telling the session where to feed the minibatch.\n",
    "    # The key of the dictionary is the placeholder node of the graph to be fed,\n",
    "    # and the value is the numpy array to feed to it.\n",
    "      feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : L2_reg}\n",
    "      _, l, predictions = session.run(\n",
    "        [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "    accuracy_val.append(accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x7f26c173b5f8>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.semilogx(regul_val, accuracy_val)\n",
    "plt.grid(True)\n",
    "plt.title('L2 regularization')\n",
    "plt.xlabel('Beta')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural network\n",
    "\n",
    "Graph for NN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128\n",
    "L2_reg = 0.001\n",
    "n_hidden = 1024 # Number of hidden nodes\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # Placehodlers for training dataset minibatches\n",
    "    tf_train_dataset = tf.placeholder(np.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(np.float32, shape=(batch_size, num_labels))\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Constants for validation and test datasets\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Define weights and biases as variables for hidden layer\n",
    "    W_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, n_hidden]))\n",
    "    b_1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "    \n",
    "    # Define weights and biases as variables for logistic regression layer\n",
    "    W_0 = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden, num_labels]))\n",
    "    b_0 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits_1 = tf.nn.relu(tf.matmul(tf_train_dataset, W_1) + b_1) # shape will be (batch_size, n_hidden)\n",
    "    \n",
    "    logits_0 = tf.matmul(logits_1, W_0) + b_0 # now the shape is (batch_size, num_labels)\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_0)\n",
    "            + beta_regul * (tf.nn.l2_loss(W_0) + tf.nn.l2_loss(W_1)))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits_0)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, W_1) + b_1), W_0) + b_0)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, W_1) + b_1), W_0) + b_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 625.662354\n",
      "Minibatch  train and validation accuracy: 10.2%, 21.0%\n",
      "Minibatch loss at step 500: 194.075836\n",
      "Minibatch  train and validation accuracy: 82.0%, 81.1%\n",
      "Minibatch loss at step 1000: 116.826126\n",
      "Minibatch  train and validation accuracy: 82.8%, 82.2%\n",
      "Minibatch loss at step 1500: 68.664917\n",
      "Minibatch  train and validation accuracy: 85.2%, 83.2%\n",
      "Minibatch loss at step 2000: 41.477554\n",
      "Minibatch  train and validation accuracy: 89.1%, 85.2%\n",
      "Minibatch loss at step 2500: 25.186268\n",
      "Minibatch  train and validation accuracy: 91.4%, 86.5%\n",
      "Minibatch loss at step 3000: 15.563068\n",
      "Minibatch  train and validation accuracy: 90.6%, 87.3%\n",
      "Test accuracy: 92.8%\n"
     ]
    }
   ],
   "source": [
    "n_steps = 3001 # number of steps to be taken\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(n_steps):\n",
    "        \n",
    "        offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        \n",
    "        # Prepare minibatch data\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : L2_reg}\n",
    "        _, l, predictions = sess.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch  train and validation accuracy: %.1f%%, %.1f%%\" \n",
    "                % (accuracy(predictions, batch_labels), \n",
    "                accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "na8xX2yHZzNF"
   },
   "source": [
    "---\n",
    "Problem 2\n",
    "---------\n",
    "Let's demonstrate an extreme case of **overfitting**. Restrict your training data to just a few batches. What happens?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # batch size reduced\n",
    "n_hidden = 1024 # Number of hidden nodes\n",
    "L2_reg = 0.000 # ratio for L2 regularization (use no regularization here)\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # Placehodlers for training dataset minibatches\n",
    "    tf_train_dataset = tf.placeholder(np.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(np.float32, shape=(batch_size, num_labels))\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Constants for validation and test datasets\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Define weights and biases as variables for hidden layer\n",
    "    W_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, n_hidden]))\n",
    "    b_1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "    \n",
    "    # Define weights and biases as variables for logistic regression layer\n",
    "    W_0 = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden, num_labels]))\n",
    "    b_0 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits_1 = tf.nn.relu(tf.matmul(tf_train_dataset, W_1) + b_1) # shape will be (batch_size, n_hidden)\n",
    "    \n",
    "    logits_0 = tf.matmul(logits_1, W_0) + b_0 # now the shape is (batch_size, num_labels)\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_0)\n",
    "            + beta_regul * (tf.nn.l2_loss(W_0) + tf.nn.l2_loss(W_1)))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits_0)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, W_1) + b_1), W_0) + b_0)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, W_1) + b_1), W_0) + b_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 303.187195\n",
      "Minibatch  train and validation accuracy: 10.2%, 24.0%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 71.2%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 71.2%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 71.2%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 71.2%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 71.2%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 71.2%\n",
      "Test accuracy: 78.3%\n"
     ]
    }
   ],
   "source": [
    "n_steps = 3001 # number of steps to be taken\n",
    "mod_batches = 3 # just use first a few batches over and over again\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(n_steps):\n",
    "        \n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # 0, 128, 0, 128, ...\n",
    "        offset = (step * batch_size) % (mod_batches * batch_size - batch_size)\n",
    "\n",
    "        # Prepare minibatch data\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : L2_reg}\n",
    "        _, l, predictions = sess.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch  train and validation accuracy: %.1f%%, %.1f%%\" \n",
    "                % (accuracy(predictions, batch_labels), \n",
    "                accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ww3SCBUdlkRc"
   },
   "source": [
    "---\n",
    "Problem 3\n",
    "---------\n",
    "Introduce Dropout on the hidden layer of the neural network. Remember: Dropout should only be introduced during training, not evaluation, otherwise your evaluation results would be stochastic as well. TensorFlow provides `nn.dropout()` for that, but you have to make sure it's only inserted during training.\n",
    "\n",
    "What happens to our extreme overfitting case?\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 128 # batch size reduced\n",
    "n_hidden = 1024 # Number of hidden nodes\n",
    "L2_reg = 0.000 # ratio for L2 regularization (use no regularization here)\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # Placehodlers for training dataset minibatches\n",
    "    tf_train_dataset = tf.placeholder(np.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(np.float32, shape=(batch_size, num_labels))\n",
    "    beta_regul = tf.placeholder(tf.float32)\n",
    "    \n",
    "    # Constants for validation and test datasets\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Define weights and biases as variables for hidden layer\n",
    "    W_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, n_hidden]))\n",
    "    b_1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "    \n",
    "    # Define weights and biases as variables for logistic regression layer\n",
    "    W_0 = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden, num_labels]))\n",
    "    b_0 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits_1 = tf.nn.relu(tf.matmul(tf_train_dataset, W_1) + b_1) # shape will be (batch_size, n_hidden)\n",
    "    \n",
    "    # DROPOUT\n",
    "    dropout_1 = tf.nn.dropout(logits_1, 0.5)\n",
    "    \n",
    "    logits_0 = tf.matmul(dropout_1, W_0) + b_0 # now the shape is (batch_size, num_labels)\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_0)\n",
    "            + beta_regul * (tf.nn.l2_loss(W_0) + tf.nn.l2_loss(W_1)))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(0.5).minimize(loss)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(logits_0)\n",
    "    valid_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, W_1) + b_1), W_0) + b_0)\n",
    "    test_prediction = tf.nn.softmax(tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, W_1) + b_1), W_0) + b_0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at step 0: 542.402039\n",
      "Minibatch  train and validation accuracy: 8.6%, 22.1%\n",
      "Minibatch loss at step 500: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 74.9%\n",
      "Minibatch loss at step 1000: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 74.4%\n",
      "Minibatch loss at step 1500: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 74.2%\n",
      "Minibatch loss at step 2000: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 74.9%\n",
      "Minibatch loss at step 2500: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 74.5%\n",
      "Minibatch loss at step 3000: 0.000000\n",
      "Minibatch  train and validation accuracy: 100.0%, 75.3%\n",
      "Test accuracy: 81.3%\n"
     ]
    }
   ],
   "source": [
    "n_steps = 3001 # number of steps to be taken\n",
    "mod_batches = 3 # just use first a few batches over and over again\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    for step in range(n_steps):\n",
    "        \n",
    "        #offset = (step * batch_size) % (train_labels.shape[0] - batch_size)\n",
    "        # 0, 128, 0, 128, ...\n",
    "        offset = (step * batch_size) % (mod_batches * batch_size - batch_size)\n",
    "\n",
    "        # Prepare minibatch data\n",
    "        batch_data = train_dataset[offset:(offset + batch_size), :]\n",
    "        batch_labels = train_labels[offset:(offset + batch_size), :]\n",
    "        \n",
    "        feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels, beta_regul : L2_reg}\n",
    "        _, l, predictions = sess.run(\n",
    "            [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        if (step % 500 == 0):\n",
    "            print(\"Minibatch loss at step %d: %f\" % (step, l))\n",
    "            print(\"Minibatch  train and validation accuracy: %.1f%%, %.1f%%\" \n",
    "                % (accuracy(predictions, batch_labels), \n",
    "                accuracy(valid_prediction.eval(), valid_labels)))\n",
    "    print(\"Test accuracy: %.1f%%\" % accuracy(test_prediction.eval(), test_labels))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-b1hTz3VWZjw"
   },
   "source": [
    "---\n",
    "Problem 4\n",
    "---------\n",
    "\n",
    "Try to get the best performance you can using a multi-layer model! The best reported test accuracy using a deep network is [97.1%](http://yaroslavvb.blogspot.com/2011/09/notmnist-dataset.html?showComment=1391023266211#c8758720086795711595).\n",
    "\n",
    "One avenue you can explore is to add multiple layers.\n",
    "\n",
    "Another one is to use learning rate decay:\n",
    "\n",
    "    global_step = tf.Variable(0)  # count the number of steps taken.\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, ...)\n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    " \n",
    " ---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with 1 hidden layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ***learning rate decay***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden = 1024 # Number of hidden nodes\n",
    "batch_size = 500\n",
    "L2_reg = 0.001 # ratio for L2 regularization\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # Placehodlers for training dataset minibatches\n",
    "    tf_train_dataset = tf.placeholder(np.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(np.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    # Constants for validation and test datasets\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Use learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.5, global_step, 1000, 0.95, staircase=True)\n",
    "    \n",
    "    # Define Weights and biases as variables for hidden layer\n",
    "    W_1 = tf.Variable(\n",
    "        tf.truncated_normal([image_size * image_size, n_hidden])\n",
    "    )\n",
    "    b_1 = tf.Variable(tf.zeros([n_hidden]))\n",
    "    \n",
    "    # Define Weights and biases as variables for logistic regression layer\n",
    "    W_0 = tf.Variable(\n",
    "        tf.truncated_normal([n_hidden, num_labels]))\n",
    "    b_0 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits_1 = tf.nn.relu(tf.matmul(tf_train_dataset, W_1) + b_1) # shape will be (batch_size, n_hidden)\n",
    "    \n",
    "    dropout_1 = tf.nn.dropout(logits_1, 0.5)\n",
    "    \n",
    "    logits_0 = tf.matmul(dropout_1, W_0) + b_0 # now the shape is (batch_size, num_labels)\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_0)\n",
    "            + L2_reg * (tf.nn.l2_loss(W_0) + tf.nn.l2_loss(W_1)))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    train_prediction = tf.nn.softmax(tf.matmul(logits_1, W_0) + b_0)\n",
    "    valid_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_valid_dataset, W_1) + b_1), W_0) + b_0)\n",
    "    test_prediction = tf.nn.softmax(\n",
    "        tf.matmul(tf.nn.relu(tf.matmul(tf_test_dataset, W_1) + b_1), W_0) + b_0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using ***early stopping*** for preventing overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at epoch 1 and iter 399: 213.277390 and learning rate: 0.500000\n",
      "Minibatch  train and validation accuracy: 77.8%, 83.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 2 and iter 799: 139.336838 and learning rate: 0.500000\n",
      "Minibatch  train and validation accuracy: 78.0%, 83.3%\n",
      "Minibatch loss at epoch 3 and iter 1199: 93.772942 and learning rate: 0.475000\n",
      "Minibatch  train and validation accuracy: 77.2%, 82.2%\n",
      "Minibatch loss at epoch 4 and iter 1599: 63.605476 and learning rate: 0.475000\n",
      "Minibatch  train and validation accuracy: 79.0%, 85.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 5 and iter 1999: 43.537388 and learning rate: 0.451250\n",
      "Minibatch  train and validation accuracy: 81.6%, 85.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 6 and iter 2399: 30.399555 and learning rate: 0.451250\n",
      "Minibatch  train and validation accuracy: 82.4%, 86.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 7 and iter 2799: 21.327969 and learning rate: 0.451250\n",
      "Minibatch  train and validation accuracy: 82.8%, 86.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 8 and iter 3199: 15.172403 and learning rate: 0.428687\n",
      "Minibatch  train and validation accuracy: 82.6%, 87.1%\n",
      "Model saved\n",
      "Minibatch loss at epoch 9 and iter 3599: 10.924870 and learning rate: 0.428687\n",
      "Minibatch  train and validation accuracy: 84.0%, 87.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 10 and iter 3999: 7.907956 and learning rate: 0.407253\n",
      "Minibatch  train and validation accuracy: 85.0%, 88.1%\n",
      "Model saved\n",
      "Minibatch loss at epoch 11 and iter 4399: 5.863419 and learning rate: 0.407253\n",
      "Minibatch  train and validation accuracy: 85.6%, 88.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 12 and iter 4799: 4.413421 and learning rate: 0.407253\n",
      "Minibatch  train and validation accuracy: 85.0%, 88.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 13 and iter 5199: 3.372710 and learning rate: 0.386890\n",
      "Minibatch  train and validation accuracy: 87.2%, 88.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 14 and iter 5599: 2.622864 and learning rate: 0.386890\n",
      "Minibatch  train and validation accuracy: 86.4%, 89.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 15 and iter 5999: 2.089647 and learning rate: 0.367546\n",
      "Minibatch  train and validation accuracy: 86.8%, 89.0%\n",
      "Minibatch loss at epoch 16 and iter 6399: 1.708904 and learning rate: 0.367546\n",
      "Minibatch  train and validation accuracy: 86.8%, 89.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 17 and iter 6799: 1.437301 and learning rate: 0.367546\n",
      "Minibatch  train and validation accuracy: 87.0%, 89.2%\n",
      "Minibatch loss at epoch 18 and iter 7199: 1.196601 and learning rate: 0.349169\n",
      "Minibatch  train and validation accuracy: 87.2%, 89.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 19 and iter 7599: 1.072147 and learning rate: 0.349169\n",
      "Minibatch  train and validation accuracy: 87.4%, 89.3%\n",
      "Minibatch loss at epoch 20 and iter 7999: 0.962896 and learning rate: 0.331710\n",
      "Minibatch  train and validation accuracy: 87.8%, 89.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 21 and iter 8399: 0.890890 and learning rate: 0.331710\n",
      "Minibatch  train and validation accuracy: 87.6%, 89.5%\n",
      "Minibatch loss at epoch 22 and iter 8799: 0.786369 and learning rate: 0.331710\n",
      "Minibatch  train and validation accuracy: 87.2%, 89.5%\n",
      "Minibatch loss at epoch 23 and iter 9199: 0.741407 and learning rate: 0.315125\n",
      "Minibatch  train and validation accuracy: 87.4%, 89.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 24 and iter 9599: 0.716637 and learning rate: 0.315125\n",
      "Minibatch  train and validation accuracy: 88.2%, 89.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 25 and iter 9999: 0.681475 and learning rate: 0.299368\n",
      "Minibatch  train and validation accuracy: 87.4%, 89.6%\n",
      "Minibatch loss at epoch 26 and iter 10399: 0.673184 and learning rate: 0.299368\n",
      "Minibatch  train and validation accuracy: 88.2%, 89.7%\n",
      "Minibatch loss at epoch 27 and iter 10799: 0.648060 and learning rate: 0.299368\n",
      "Minibatch  train and validation accuracy: 87.4%, 89.7%\n",
      "Minibatch loss at epoch 28 and iter 11199: 0.640296 and learning rate: 0.284400\n",
      "Minibatch  train and validation accuracy: 87.2%, 89.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 29 and iter 11599: 0.626379 and learning rate: 0.284400\n",
      "Minibatch  train and validation accuracy: 87.2%, 89.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 30 and iter 11999: 0.607407 and learning rate: 0.270180\n",
      "Minibatch  train and validation accuracy: 86.8%, 90.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 31 and iter 12399: 0.594732 and learning rate: 0.270180\n",
      "Minibatch  train and validation accuracy: 87.0%, 89.9%\n",
      "Minibatch loss at epoch 32 and iter 12799: 0.600053 and learning rate: 0.270180\n",
      "Minibatch  train and validation accuracy: 87.2%, 89.9%\n",
      "Minibatch loss at epoch 33 and iter 13199: 0.586643 and learning rate: 0.256671\n",
      "Minibatch  train and validation accuracy: 87.2%, 90.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 34 and iter 13599: 0.572637 and learning rate: 0.256671\n",
      "Minibatch  train and validation accuracy: 87.2%, 89.9%\n",
      "Minibatch loss at epoch 35 and iter 13999: 0.585307 and learning rate: 0.243837\n",
      "Minibatch  train and validation accuracy: 87.2%, 90.0%\n",
      "Minibatch loss at epoch 36 and iter 14399: 0.589853 and learning rate: 0.243837\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.0%\n",
      "Minibatch loss at epoch 37 and iter 14799: 0.569092 and learning rate: 0.243837\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 38 and iter 15199: 0.572517 and learning rate: 0.231646\n",
      "Minibatch  train and validation accuracy: 87.0%, 90.0%\n",
      "Minibatch loss at epoch 39 and iter 15599: 0.558442 and learning rate: 0.231646\n",
      "Minibatch  train and validation accuracy: 87.4%, 90.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 40 and iter 15999: 0.579519 and learning rate: 0.220063\n",
      "Minibatch  train and validation accuracy: 87.4%, 90.0%\n",
      "Minibatch loss at epoch 41 and iter 16399: 0.570925 and learning rate: 0.220063\n",
      "Minibatch  train and validation accuracy: 87.4%, 90.1%\n",
      "Minibatch loss at epoch 42 and iter 16799: 0.563078 and learning rate: 0.220063\n",
      "Minibatch  train and validation accuracy: 87.4%, 90.1%\n",
      "Minibatch loss at epoch 43 and iter 17199: 0.568736 and learning rate: 0.209060\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 44 and iter 17599: 0.564323 and learning rate: 0.209060\n",
      "Minibatch  train and validation accuracy: 87.4%, 90.1%\n",
      "Minibatch loss at epoch 45 and iter 17999: 0.564884 and learning rate: 0.198607\n",
      "Minibatch  train and validation accuracy: 87.2%, 90.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 46 and iter 18399: 0.538346 and learning rate: 0.198607\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.1%\n",
      "Minibatch loss at epoch 47 and iter 18799: 0.559709 and learning rate: 0.198607\n",
      "Minibatch  train and validation accuracy: 87.6%, 90.1%\n",
      "Minibatch loss at epoch 48 and iter 19199: 0.561630 and learning rate: 0.188677\n",
      "Minibatch  train and validation accuracy: 87.4%, 90.0%\n",
      "Minibatch loss at epoch 49 and iter 19599: 0.545569 and learning rate: 0.188677\n",
      "Minibatch  train and validation accuracy: 87.6%, 90.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 50 and iter 19999: 0.552006 and learning rate: 0.179243\n",
      "Minibatch  train and validation accuracy: 87.4%, 90.1%\n",
      "Minibatch loss at epoch 51 and iter 20399: 0.576867 and learning rate: 0.179243\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.2%\n",
      "Minibatch loss at epoch 52 and iter 20799: 0.555272 and learning rate: 0.179243\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.2%\n",
      "Minibatch loss at epoch 53 and iter 21199: 0.540717 and learning rate: 0.170281\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.2%\n",
      "Minibatch loss at epoch 54 and iter 21599: 0.570635 and learning rate: 0.170281\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.1%\n",
      "Minibatch loss at epoch 55 and iter 21999: 0.565294 and learning rate: 0.161767\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.2%\n",
      "Minibatch loss at epoch 56 and iter 22399: 0.554863 and learning rate: 0.161767\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.2%\n",
      "Minibatch loss at epoch 57 and iter 22799: 0.560074 and learning rate: 0.161767\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 58 and iter 23199: 0.553362 and learning rate: 0.153678\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.2%\n",
      "Minibatch loss at epoch 59 and iter 23599: 0.575065 and learning rate: 0.153678\n",
      "Minibatch  train and validation accuracy: 87.6%, 90.2%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 60 and iter 23999: 0.541067 and learning rate: 0.145994\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.2%\n",
      "Minibatch loss at epoch 61 and iter 24399: 0.557529 and learning rate: 0.145994\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.3%\n",
      "Minibatch loss at epoch 62 and iter 24799: 0.540887 and learning rate: 0.145994\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.2%\n",
      "Minibatch loss at epoch 63 and iter 25199: 0.567110 and learning rate: 0.138695\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.3%\n",
      "Minibatch loss at epoch 64 and iter 25599: 0.541678 and learning rate: 0.138695\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 65 and iter 25999: 0.545184 and learning rate: 0.131760\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.3%\n",
      "Minibatch loss at epoch 66 and iter 26399: 0.535162 and learning rate: 0.131760\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.2%\n",
      "Minibatch loss at epoch 67 and iter 26799: 0.533997 and learning rate: 0.131760\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.3%\n",
      "Minibatch loss at epoch 68 and iter 27199: 0.535577 and learning rate: 0.125172\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.3%\n",
      "Minibatch loss at epoch 69 and iter 27599: 0.529813 and learning rate: 0.125172\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.3%\n",
      "Minibatch loss at epoch 70 and iter 27999: 0.552042 and learning rate: 0.118913\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 71 and iter 28399: 0.537874 and learning rate: 0.118913\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.2%\n",
      "Minibatch loss at epoch 72 and iter 28799: 0.535182 and learning rate: 0.118913\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.3%\n",
      "Minibatch loss at epoch 73 and iter 29199: 0.543705 and learning rate: 0.112968\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.3%\n",
      "Minibatch loss at epoch 74 and iter 29599: 0.532103 and learning rate: 0.112968\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.3%\n",
      "Minibatch loss at epoch 75 and iter 29999: 0.542132 and learning rate: 0.107319\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.4%\n",
      "Minibatch loss at epoch 76 and iter 30399: 0.536177 and learning rate: 0.107319\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 77 and iter 30799: 0.544634 and learning rate: 0.107319\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 78 and iter 31199: 0.536127 and learning rate: 0.101953\n",
      "Minibatch  train and validation accuracy: 87.6%, 90.3%\n",
      "Minibatch loss at epoch 79 and iter 31599: 0.532515 and learning rate: 0.101953\n",
      "Minibatch  train and validation accuracy: 87.8%, 90.4%\n",
      "Minibatch loss at epoch 80 and iter 31999: 0.539343 and learning rate: 0.096856\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.3%\n",
      "Minibatch loss at epoch 81 and iter 32399: 0.533100 and learning rate: 0.096856\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.3%\n",
      "Minibatch loss at epoch 82 and iter 32799: 0.550764 and learning rate: 0.096856\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.4%\n",
      "Minibatch loss at epoch 83 and iter 33199: 0.542274 and learning rate: 0.092013\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.4%\n",
      "Minibatch loss at epoch 84 and iter 33599: 0.537822 and learning rate: 0.092013\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.3%\n",
      "Minibatch loss at epoch 85 and iter 33999: 0.530875 and learning rate: 0.087412\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.3%\n",
      "Minibatch loss at epoch 86 and iter 34399: 0.550596 and learning rate: 0.087412\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 87 and iter 34799: 0.519106 and learning rate: 0.087412\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.3%\n",
      "Minibatch loss at epoch 88 and iter 35199: 0.535260 and learning rate: 0.083042\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 89 and iter 35599: 0.531807 and learning rate: 0.083042\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.3%\n",
      "Minibatch loss at epoch 90 and iter 35999: 0.535793 and learning rate: 0.078890\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.3%\n",
      "Minibatch loss at epoch 91 and iter 36399: 0.539417 and learning rate: 0.078890\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.4%\n",
      "Minibatch loss at epoch 92 and iter 36799: 0.533863 and learning rate: 0.078890\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.4%\n",
      "Minibatch loss at epoch 93 and iter 37199: 0.535428 and learning rate: 0.074945\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.3%\n",
      "Minibatch loss at epoch 94 and iter 37599: 0.523564 and learning rate: 0.074945\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 95 and iter 37999: 0.551528 and learning rate: 0.071198\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.4%\n",
      "Minibatch loss at epoch 96 and iter 38399: 0.536979 and learning rate: 0.071198\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 97 and iter 38799: 0.521571 and learning rate: 0.071198\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.3%\n",
      "Minibatch loss at epoch 98 and iter 39199: 0.532873 and learning rate: 0.067638\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 99 and iter 39599: 0.534616 and learning rate: 0.067638\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 100 and iter 39999: 0.551888 and learning rate: 0.064256\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.3%\n",
      "Minibatch loss at epoch 101 and iter 40399: 0.548608 and learning rate: 0.064256\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.4%\n",
      "Minibatch loss at epoch 102 and iter 40799: 0.506267 and learning rate: 0.064256\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 103 and iter 41199: 0.532333 and learning rate: 0.061043\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 104 and iter 41599: 0.518446 and learning rate: 0.061043\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 105 and iter 41999: 0.527422 and learning rate: 0.057991\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 106 and iter 42399: 0.516987 and learning rate: 0.057991\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 107 and iter 42799: 0.520286 and learning rate: 0.057991\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 108 and iter 43199: 0.538985 and learning rate: 0.055092\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.3%\n",
      "Minibatch loss at epoch 109 and iter 43599: 0.532782 and learning rate: 0.055092\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.3%\n",
      "Minibatch loss at epoch 110 and iter 43999: 0.535323 and learning rate: 0.052337\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.3%\n",
      "Minibatch loss at epoch 111 and iter 44399: 0.534999 and learning rate: 0.052337\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 112 and iter 44799: 0.549645 and learning rate: 0.052337\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 113 and iter 45199: 0.541342 and learning rate: 0.049720\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 114 and iter 45599: 0.530859 and learning rate: 0.049720\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 115 and iter 45999: 0.530459 and learning rate: 0.047234\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.5%\n",
      "Minibatch loss at epoch 116 and iter 46399: 0.514229 and learning rate: 0.047234\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 117 and iter 46799: 0.530154 and learning rate: 0.047234\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.5%\n",
      "Minibatch loss at epoch 118 and iter 47199: 0.510990 and learning rate: 0.044872\n",
      "Minibatch  train and validation accuracy: 88.0%, 90.4%\n",
      "Minibatch loss at epoch 119 and iter 47599: 0.528630 and learning rate: 0.044872\n",
      "Minibatch  train and validation accuracy: 88.2%, 90.4%\n",
      "Minibatch loss at epoch 120 and iter 47999: 0.541361 and learning rate: 0.042629\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 121 and iter 48399: 0.533165 and learning rate: 0.042629\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 122 and iter 48799: 0.535274 and learning rate: 0.042629\n",
      "Minibatch  train and validation accuracy: 88.8%, 90.4%\n",
      "Minibatch loss at epoch 123 and iter 49199: 0.528526 and learning rate: 0.040497\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 124 and iter 49599: 0.541369 and learning rate: 0.040497\n",
      "Minibatch  train and validation accuracy: 88.8%, 90.5%\n",
      "Minibatch loss at epoch 125 and iter 49999: 0.541893 and learning rate: 0.038472\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 126 and iter 50399: 0.521251 and learning rate: 0.038472\n",
      "Minibatch  train and validation accuracy: 88.8%, 90.4%\n",
      "Minibatch loss at epoch 127 and iter 50799: 0.522927 and learning rate: 0.038472\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.5%\n",
      "Minibatch loss at epoch 128 and iter 51199: 0.536293 and learning rate: 0.036549\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.5%\n",
      "Minibatch loss at epoch 129 and iter 51599: 0.538707 and learning rate: 0.036549\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 130 and iter 51999: 0.512637 and learning rate: 0.034721\n",
      "Minibatch  train and validation accuracy: 88.8%, 90.5%\n",
      "Minibatch loss at epoch 131 and iter 52399: 0.532885 and learning rate: 0.034721\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.5%\n",
      "Minibatch loss at epoch 132 and iter 52799: 0.533034 and learning rate: 0.034721\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 133 and iter 53199: 0.513797 and learning rate: 0.032985\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.5%\n",
      "Minibatch loss at epoch 134 and iter 53599: 0.536875 and learning rate: 0.032985\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.5%\n",
      "Minibatch loss at epoch 135 and iter 53999: 0.536478 and learning rate: 0.031336\n",
      "Minibatch  train and validation accuracy: 88.8%, 90.5%\n",
      "Minibatch loss at epoch 136 and iter 54399: 0.528813 and learning rate: 0.031336\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 137 and iter 54799: 0.535905 and learning rate: 0.031336\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 138 and iter 55199: 0.520428 and learning rate: 0.029769\n",
      "Minibatch  train and validation accuracy: 88.8%, 90.4%\n",
      "Minibatch loss at epoch 139 and iter 55599: 0.517406 and learning rate: 0.029769\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 140 and iter 55999: 0.535567 and learning rate: 0.028281\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 141 and iter 56399: 0.515028 and learning rate: 0.028281\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 142 and iter 56799: 0.526025 and learning rate: 0.028281\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.5%\n",
      "Minibatch loss at epoch 143 and iter 57199: 0.524216 and learning rate: 0.026867\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 144 and iter 57599: 0.524528 and learning rate: 0.026867\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 145 and iter 57999: 0.528901 and learning rate: 0.025523\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 146 and iter 58399: 0.523393 and learning rate: 0.025523\n",
      "Minibatch  train and validation accuracy: 88.8%, 90.4%\n",
      "Minibatch loss at epoch 147 and iter 58799: 0.532286 and learning rate: 0.025523\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 148 and iter 59199: 0.527231 and learning rate: 0.024247\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Minibatch loss at epoch 149 and iter 59599: 0.526802 and learning rate: 0.024247\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 150 and iter 59999: 0.527940 and learning rate: 0.023035\n",
      "Minibatch  train and validation accuracy: 88.4%, 90.4%\n",
      "Minibatch loss at epoch 151 and iter 60399: 0.527495 and learning rate: 0.023035\n",
      "Minibatch  train and validation accuracy: 88.6%, 90.4%\n",
      "Final Test accuracy: 95.4%\n",
      "Test accuracy with the best model: 95.5%\n",
      "Total run time 2.9170 minutes\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200 # number of epochs\n",
    "\n",
    "# early-stopping parameters\n",
    "patience = 5000 # look as this many examples regardless\n",
    "patience_increase = 2\n",
    "improvement_threshold = 0.995 # a relative improvement of \n",
    "                                  # this much is considered significant\n",
    "best_valid_loss = np.inf\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "n_train_batches = train_dataset.shape[0] // batch_size\n",
    "\n",
    "valid_freq = min(n_train_batches, patience // 2)\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            \n",
    "            batch_data = \\\n",
    "                train_dataset[minibatch_index * batch_size:(minibatch_index + 1) * batch_size, :]\n",
    "            batch_labels = \\\n",
    "                train_labels[minibatch_index * batch_size:(minibatch_index + 1) * batch_size, :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = sess.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index # cumulative iteration number\n",
    "        \n",
    "            if (iter + 1) % valid_freq == 0:\n",
    "                this_valid_loss = 1. - (accuracy(valid_prediction.eval(), valid_labels) / 100.)\n",
    "            \n",
    "                print(\"Minibatch loss at epoch %i and iter %i: %f and learning rate: %f\" % \n",
    "                      (epoch, iter, l, sess.run(learning_rate)))\n",
    "                print(\"Minibatch  train and validation accuracy: %.1f%%, %.1f%%\" \n",
    "                    % (accuracy(predictions, batch_labels), \n",
    "                    accuracy(valid_prediction.eval(), valid_labels)))\n",
    "            \n",
    "                if this_valid_loss < best_valid_loss:\n",
    "                    if this_valid_loss < best_valid_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                    \n",
    "                    best_valid_loss = this_valid_loss\n",
    "                    \n",
    "                    params = (sess.run(W_0), sess.run(b_0),\n",
    "                              sess.run(W_1), sess.run(b_1))\n",
    "                    \n",
    "                    # save the best model\n",
    "                    with open('best_model_params.pkl', 'wb') as f:\n",
    "                            pickle.dump(params, f)\n",
    "                    print('Model saved')\n",
    "        \n",
    "            if patience <= iter:\n",
    "                    done_looping = True\n",
    "                    break\n",
    "                    \n",
    "    print(\"Final Test accuracy: %.1f%%\" \n",
    "                  % accuracy(test_prediction.eval(), test_labels))\n",
    "    \n",
    "    with open('best_model_params.pkl', 'rb') as f:\n",
    "        W_0_best, b_0_best, W_1_best, b_1_best = pickle.load(f)\n",
    "        \n",
    "    W_0_init, b_0_init = tf.assign(W_0, W_0_best), tf.assign(b_0, b_0_best)\n",
    "    W_1_init, b_1_init = tf.assign(W_1, W_1_best), tf.assign(b_1, b_1_best)\n",
    "    \n",
    "    sess.run([W_0_init, b_0_init, W_1_init, b_1_init])\n",
    "    \n",
    "    print(\"Test accuracy with the best model: %.1f%%\" \n",
    "                  % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    print('Total run time %.4f minutes' % ((end_time - start_time) / 60.))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model with two hidden layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's try a model with 2 hidden layers. \n",
    "\n",
    "Weights with truncated normal distribution won't work here, thus we will initialize the weights with [Xavier initialization](https://prateekvjoshi.com/2016/03/29/understanding-xavier-initialization-in-deep-neural-networks/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_hidden_2 = 1024 # Number of hidden nodes for hidden layer 2\n",
    "n_hidden_1 = 500  # Number of hidden nodes for hidden layer 1\n",
    "batch_size = 128\n",
    "L2_reg = 0.000 # ratio for L2 regularization\n",
    "\n",
    "g = tf.Graph()\n",
    "with g.as_default():\n",
    "    \n",
    "    # Placehodlers for training dataset minibatches\n",
    "    tf_train_dataset = tf.placeholder(np.float32, shape=(batch_size, image_size * image_size))\n",
    "    tf_train_labels = tf.placeholder(np.float32, shape=(batch_size, num_labels))\n",
    "    \n",
    "    # Constants for validation and test datasets\n",
    "    tf_valid_dataset = tf.constant(valid_dataset)\n",
    "    tf_test_dataset = tf.constant(test_dataset)\n",
    "    \n",
    "    # Use learning rate decay\n",
    "    global_step = tf.Variable(0)\n",
    "    learning_rate = tf.train.exponential_decay(0.05, global_step, 5000, 0.95, staircase=True)\n",
    "    \n",
    "    # Define Weights and biases as variables for hidden layers\n",
    "    W_2 = tf.get_variable(\"W2\", shape=[image_size * image_size, n_hidden_2],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_2 = tf.Variable(tf.zeros([n_hidden_2]))\n",
    "    \n",
    "    W_1 = tf.get_variable(\"W1\", shape=[n_hidden_2, n_hidden_1],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_1 = tf.Variable(tf.zeros([n_hidden_1]))\n",
    "    \n",
    "    # Define Weights and biases as variables for logistic regression layer\n",
    "    W_0 = tf.get_variable(\"W0\", shape=[n_hidden_1, num_labels],\n",
    "                                initializer=tf.contrib.layers.xavier_initializer())\n",
    "    b_0 = tf.Variable(tf.zeros([num_labels]))\n",
    "    \n",
    "    logits_2 = tf.nn.relu(tf.matmul(tf_train_dataset, W_2) + b_2) # shape will be (batch_size, n_hidden_2)\n",
    "    \n",
    "    dropout_2 = tf.nn.dropout(logits_2, 0.5)\n",
    "    \n",
    "    logits_1 = tf.nn.relu(tf.matmul(dropout_2, W_1) + b_1) # shape will be (batch_size, n_hidden_1)\n",
    "    \n",
    "    dropout_1 = tf.nn.dropout(logits_1, 0.5)\n",
    "    \n",
    "    logits_0 = tf.matmul(dropout_1, W_0) + b_0 # now the shape is (batch_size, num_labels)\n",
    "    \n",
    "    loss = tf.reduce_mean(\n",
    "            tf.nn.softmax_cross_entropy_with_logits(labels=tf_train_labels, logits=logits_0)\n",
    "            + L2_reg * (tf.nn.l2_loss(W_0) + tf.nn.l2_loss(W_1) + tf.nn.l2_loss(W_2)))\n",
    "    \n",
    "    optimizer = tf.train.GradientDescentOptimizer(learning_rate).minimize(loss, global_step=global_step)\n",
    "    \n",
    "    # Predictions\n",
    "    def get_logits(dataset):\n",
    "        get_logits_2 = tf.nn.relu(tf.matmul(dataset, W_2) + b_2)\n",
    "        get_logits_1 = tf.nn.relu(tf.matmul(get_logits_2, W_1) + b_1)\n",
    "        get_logits_0 = tf.matmul(get_logits_1, W_0) + b_0\n",
    "        return get_logits_0\n",
    "\n",
    "    train_prediction = tf.nn.softmax(get_logits(tf_train_dataset))\n",
    "    valid_prediction = tf.nn.softmax(get_logits(tf_valid_dataset))\n",
    "    test_prediction = tf.nn.softmax(get_logits(tf_test_dataset))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initialized\n",
      "Minibatch loss at epoch 1 and iter 1561: 0.674343 and learning rate: 0.050000\n",
      "Minibatch  train and validation accuracy: 83.6%, 84.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 2 and iter 3123: 0.633871 and learning rate: 0.050000\n",
      "Minibatch  train and validation accuracy: 84.4%, 86.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 3 and iter 4685: 0.570524 and learning rate: 0.050000\n",
      "Minibatch  train and validation accuracy: 84.4%, 87.1%\n",
      "Model saved\n",
      "Minibatch loss at epoch 4 and iter 6247: 0.530805 and learning rate: 0.047500\n",
      "Minibatch  train and validation accuracy: 84.4%, 87.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 5 and iter 7809: 0.603892 and learning rate: 0.047500\n",
      "Minibatch  train and validation accuracy: 84.4%, 87.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 6 and iter 9371: 0.469438 and learning rate: 0.047500\n",
      "Minibatch  train and validation accuracy: 85.2%, 88.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 7 and iter 10933: 0.484484 and learning rate: 0.045125\n",
      "Minibatch  train and validation accuracy: 85.2%, 88.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 8 and iter 12495: 0.490401 and learning rate: 0.045125\n",
      "Minibatch  train and validation accuracy: 85.2%, 88.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 9 and iter 14057: 0.465474 and learning rate: 0.045125\n",
      "Minibatch  train and validation accuracy: 84.4%, 88.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 10 and iter 15619: 0.502953 and learning rate: 0.042869\n",
      "Minibatch  train and validation accuracy: 85.2%, 89.1%\n",
      "Model saved\n",
      "Minibatch loss at epoch 11 and iter 17181: 0.477176 and learning rate: 0.042869\n",
      "Minibatch  train and validation accuracy: 85.9%, 89.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 12 and iter 18743: 0.534018 and learning rate: 0.042869\n",
      "Minibatch  train and validation accuracy: 85.9%, 89.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 13 and iter 20305: 0.515423 and learning rate: 0.040725\n",
      "Minibatch  train and validation accuracy: 85.9%, 89.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 14 and iter 21867: 0.434670 and learning rate: 0.040725\n",
      "Minibatch  train and validation accuracy: 85.2%, 89.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 15 and iter 23429: 0.443660 and learning rate: 0.040725\n",
      "Minibatch  train and validation accuracy: 85.9%, 89.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 16 and iter 24991: 0.409259 and learning rate: 0.040725\n",
      "Minibatch  train and validation accuracy: 88.3%, 89.9%\n",
      "Minibatch loss at epoch 17 and iter 26553: 0.470720 and learning rate: 0.038689\n",
      "Minibatch  train and validation accuracy: 87.5%, 90.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 18 and iter 28115: 0.481847 and learning rate: 0.038689\n",
      "Minibatch  train and validation accuracy: 87.5%, 90.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 19 and iter 29677: 0.392947 and learning rate: 0.038689\n",
      "Minibatch  train and validation accuracy: 89.1%, 90.1%\n",
      "Minibatch loss at epoch 20 and iter 31239: 0.396937 and learning rate: 0.036755\n",
      "Minibatch  train and validation accuracy: 86.7%, 90.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 21 and iter 32801: 0.421864 and learning rate: 0.036755\n",
      "Minibatch  train and validation accuracy: 89.1%, 90.2%\n",
      "Minibatch loss at epoch 22 and iter 34363: 0.400586 and learning rate: 0.036755\n",
      "Minibatch  train and validation accuracy: 88.3%, 90.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 23 and iter 35925: 0.414252 and learning rate: 0.034917\n",
      "Minibatch  train and validation accuracy: 88.3%, 90.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 24 and iter 37487: 0.402840 and learning rate: 0.034917\n",
      "Minibatch  train and validation accuracy: 87.5%, 90.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 25 and iter 39049: 0.444182 and learning rate: 0.034917\n",
      "Minibatch  train and validation accuracy: 88.3%, 90.4%\n",
      "Minibatch loss at epoch 26 and iter 40611: 0.412398 and learning rate: 0.033171\n",
      "Minibatch  train and validation accuracy: 87.5%, 90.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 27 and iter 42173: 0.396589 and learning rate: 0.033171\n",
      "Minibatch  train and validation accuracy: 88.3%, 90.6%\n",
      "Minibatch loss at epoch 28 and iter 43735: 0.416269 and learning rate: 0.033171\n",
      "Minibatch  train and validation accuracy: 89.8%, 90.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 29 and iter 45297: 0.424493 and learning rate: 0.031512\n",
      "Minibatch  train and validation accuracy: 87.5%, 90.6%\n",
      "Minibatch loss at epoch 30 and iter 46859: 0.435116 and learning rate: 0.031512\n",
      "Minibatch  train and validation accuracy: 89.8%, 90.7%\n",
      "Minibatch loss at epoch 31 and iter 48421: 0.367155 and learning rate: 0.031512\n",
      "Minibatch  train and validation accuracy: 90.6%, 90.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 32 and iter 49983: 0.363108 and learning rate: 0.031512\n",
      "Minibatch  train and validation accuracy: 91.4%, 90.8%\n",
      "Minibatch loss at epoch 33 and iter 51545: 0.374165 and learning rate: 0.029937\n",
      "Minibatch  train and validation accuracy: 89.8%, 90.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 34 and iter 53107: 0.437676 and learning rate: 0.029937\n",
      "Minibatch  train and validation accuracy: 90.6%, 90.8%\n",
      "Minibatch loss at epoch 35 and iter 54669: 0.393676 and learning rate: 0.029937\n",
      "Minibatch  train and validation accuracy: 91.4%, 90.8%\n",
      "Minibatch loss at epoch 36 and iter 56231: 0.403115 and learning rate: 0.028440\n",
      "Minibatch  train and validation accuracy: 91.4%, 90.8%\n",
      "Minibatch loss at epoch 37 and iter 57793: 0.382341 and learning rate: 0.028440\n",
      "Minibatch  train and validation accuracy: 89.1%, 90.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 38 and iter 59355: 0.354013 and learning rate: 0.028440\n",
      "Minibatch  train and validation accuracy: 90.6%, 91.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 39 and iter 60917: 0.383217 and learning rate: 0.027018\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 40 and iter 62479: 0.305492 and learning rate: 0.027018\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.0%\n",
      "Model saved\n",
      "Minibatch loss at epoch 41 and iter 64041: 0.344783 and learning rate: 0.027018\n",
      "Minibatch  train and validation accuracy: 90.6%, 91.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 42 and iter 65603: 0.350540 and learning rate: 0.025667\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.1%\n",
      "Minibatch loss at epoch 43 and iter 67165: 0.374839 and learning rate: 0.025667\n",
      "Minibatch  train and validation accuracy: 90.6%, 91.1%\n",
      "Minibatch loss at epoch 44 and iter 68727: 0.391296 and learning rate: 0.025667\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.1%\n",
      "Minibatch loss at epoch 45 and iter 70289: 0.363128 and learning rate: 0.024384\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.2%\n",
      "Minibatch loss at epoch 46 and iter 71851: 0.343812 and learning rate: 0.024384\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.1%\n",
      "Minibatch loss at epoch 47 and iter 73413: 0.343155 and learning rate: 0.024384\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.1%\n",
      "Minibatch loss at epoch 48 and iter 74975: 0.314233 and learning rate: 0.024384\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.1%\n",
      "Minibatch loss at epoch 49 and iter 76537: 0.352289 and learning rate: 0.023165\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.1%\n",
      "Minibatch loss at epoch 50 and iter 78099: 0.316998 and learning rate: 0.023165\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.1%\n",
      "Minibatch loss at epoch 51 and iter 79661: 0.334452 and learning rate: 0.023165\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.2%\n",
      "Minibatch loss at epoch 52 and iter 81223: 0.365585 and learning rate: 0.022006\n",
      "Minibatch  train and validation accuracy: 89.8%, 91.1%\n",
      "Minibatch loss at epoch 53 and iter 82785: 0.361200 and learning rate: 0.022006\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.2%\n",
      "Model saved\n",
      "Minibatch loss at epoch 54 and iter 84347: 0.360731 and learning rate: 0.022006\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.1%\n",
      "Minibatch loss at epoch 55 and iter 85909: 0.273157 and learning rate: 0.020906\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.2%\n",
      "Minibatch loss at epoch 56 and iter 87471: 0.330010 and learning rate: 0.020906\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 57 and iter 89033: 0.281463 and learning rate: 0.020906\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.3%\n",
      "Model saved\n",
      "Minibatch loss at epoch 58 and iter 90595: 0.359466 and learning rate: 0.019861\n",
      "Minibatch  train and validation accuracy: 90.6%, 91.1%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 59 and iter 92157: 0.400586 and learning rate: 0.019861\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.2%\n",
      "Minibatch loss at epoch 60 and iter 93719: 0.327962 and learning rate: 0.019861\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 61 and iter 95281: 0.318761 and learning rate: 0.018868\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.2%\n",
      "Minibatch loss at epoch 62 and iter 96843: 0.316944 and learning rate: 0.018868\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.2%\n",
      "Minibatch loss at epoch 63 and iter 98405: 0.339235 and learning rate: 0.018868\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.3%\n",
      "Minibatch loss at epoch 64 and iter 99967: 0.302327 and learning rate: 0.018868\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.3%\n",
      "Minibatch loss at epoch 65 and iter 101529: 0.408892 and learning rate: 0.017924\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.3%\n",
      "Minibatch loss at epoch 66 and iter 103091: 0.297056 and learning rate: 0.017924\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.3%\n",
      "Minibatch loss at epoch 67 and iter 104653: 0.377751 and learning rate: 0.017924\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 68 and iter 106215: 0.315090 and learning rate: 0.017028\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.3%\n",
      "Minibatch loss at epoch 69 and iter 107777: 0.301341 and learning rate: 0.017028\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.3%\n",
      "Minibatch loss at epoch 70 and iter 109339: 0.288858 and learning rate: 0.017028\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.4%\n",
      "Model saved\n",
      "Minibatch loss at epoch 71 and iter 110901: 0.323283 and learning rate: 0.016177\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.3%\n",
      "Minibatch loss at epoch 72 and iter 112463: 0.344079 and learning rate: 0.016177\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.4%\n",
      "Minibatch loss at epoch 73 and iter 114025: 0.342427 and learning rate: 0.016177\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.5%\n",
      "Model saved\n",
      "Minibatch loss at epoch 74 and iter 115587: 0.236884 and learning rate: 0.015368\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.5%\n",
      "Minibatch loss at epoch 75 and iter 117149: 0.351026 and learning rate: 0.015368\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.3%\n",
      "Minibatch loss at epoch 76 and iter 118711: 0.290111 and learning rate: 0.015368\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.4%\n",
      "Minibatch loss at epoch 77 and iter 120273: 0.345871 and learning rate: 0.014599\n",
      "Minibatch  train and validation accuracy: 91.4%, 91.4%\n",
      "Minibatch loss at epoch 78 and iter 121835: 0.379230 and learning rate: 0.014599\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.5%\n",
      "Minibatch loss at epoch 79 and iter 123397: 0.318004 and learning rate: 0.014599\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.4%\n",
      "Minibatch loss at epoch 80 and iter 124959: 0.286732 and learning rate: 0.014599\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.4%\n",
      "Minibatch loss at epoch 81 and iter 126521: 0.317018 and learning rate: 0.013869\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.3%\n",
      "Minibatch loss at epoch 82 and iter 128083: 0.268531 and learning rate: 0.013869\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.5%\n",
      "Minibatch loss at epoch 83 and iter 129645: 0.289186 and learning rate: 0.013869\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.5%\n",
      "Minibatch loss at epoch 84 and iter 131207: 0.272174 and learning rate: 0.013176\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.5%\n",
      "Minibatch loss at epoch 85 and iter 132769: 0.383589 and learning rate: 0.013176\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.5%\n",
      "Minibatch loss at epoch 86 and iter 134331: 0.277759 and learning rate: 0.013176\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 87 and iter 135893: 0.356529 and learning rate: 0.012517\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.6%\n",
      "Model saved\n",
      "Minibatch loss at epoch 88 and iter 137455: 0.400139 and learning rate: 0.012517\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.5%\n",
      "Minibatch loss at epoch 89 and iter 139017: 0.239858 and learning rate: 0.012517\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.6%\n",
      "Minibatch loss at epoch 90 and iter 140579: 0.345888 and learning rate: 0.011891\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.5%\n",
      "Minibatch loss at epoch 91 and iter 142141: 0.354417 and learning rate: 0.011891\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.6%\n",
      "Minibatch loss at epoch 92 and iter 143703: 0.343557 and learning rate: 0.011891\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.5%\n",
      "Minibatch loss at epoch 93 and iter 145265: 0.314365 and learning rate: 0.011297\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.4%\n",
      "Minibatch loss at epoch 94 and iter 146827: 0.362845 and learning rate: 0.011297\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.5%\n",
      "Minibatch loss at epoch 95 and iter 148389: 0.292332 and learning rate: 0.011297\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.5%\n",
      "Minibatch loss at epoch 96 and iter 149951: 0.283821 and learning rate: 0.011297\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.6%\n",
      "Minibatch loss at epoch 97 and iter 151513: 0.296531 and learning rate: 0.010732\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.5%\n",
      "Minibatch loss at epoch 98 and iter 153075: 0.319072 and learning rate: 0.010732\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.6%\n",
      "Minibatch loss at epoch 99 and iter 154637: 0.303969 and learning rate: 0.010732\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.6%\n",
      "Minibatch loss at epoch 100 and iter 156199: 0.238549 and learning rate: 0.010195\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.6%\n",
      "Minibatch loss at epoch 101 and iter 157761: 0.270253 and learning rate: 0.010195\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 102 and iter 159323: 0.236642 and learning rate: 0.010195\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.6%\n",
      "Minibatch loss at epoch 103 and iter 160885: 0.233700 and learning rate: 0.009686\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.5%\n",
      "Minibatch loss at epoch 104 and iter 162447: 0.306593 and learning rate: 0.009686\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.5%\n",
      "Minibatch loss at epoch 105 and iter 164009: 0.286647 and learning rate: 0.009686\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.6%\n",
      "Minibatch loss at epoch 106 and iter 165571: 0.313597 and learning rate: 0.009201\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.6%\n",
      "Minibatch loss at epoch 107 and iter 167133: 0.307940 and learning rate: 0.009201\n",
      "Minibatch  train and validation accuracy: 92.2%, 91.6%\n",
      "Minibatch loss at epoch 108 and iter 168695: 0.349281 and learning rate: 0.009201\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.7%\n",
      "Model saved\n",
      "Minibatch loss at epoch 109 and iter 170257: 0.268110 and learning rate: 0.008741\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.6%\n",
      "Minibatch loss at epoch 110 and iter 171819: 0.265536 and learning rate: 0.008741\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.6%\n",
      "Minibatch loss at epoch 111 and iter 173381: 0.252134 and learning rate: 0.008741\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 112 and iter 174943: 0.340615 and learning rate: 0.008741\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.7%\n",
      "Minibatch loss at epoch 113 and iter 176505: 0.322916 and learning rate: 0.008304\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.7%\n",
      "Minibatch loss at epoch 114 and iter 178067: 0.220281 and learning rate: 0.008304\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.6%\n",
      "Minibatch loss at epoch 115 and iter 179629: 0.355263 and learning rate: 0.008304\n",
      "Minibatch  train and validation accuracy: 93.0%, 91.7%\n",
      "Minibatch loss at epoch 116 and iter 181191: 0.270264 and learning rate: 0.007889\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.7%\n",
      "Minibatch loss at epoch 117 and iter 182753: 0.343035 and learning rate: 0.007889\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 118 and iter 184315: 0.281507 and learning rate: 0.007889\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.8%\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 119 and iter 185877: 0.389125 and learning rate: 0.007495\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.7%\n",
      "Minibatch loss at epoch 120 and iter 187439: 0.292446 and learning rate: 0.007495\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.7%\n",
      "Minibatch loss at epoch 121 and iter 189001: 0.245974 and learning rate: 0.007495\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.7%\n",
      "Minibatch loss at epoch 122 and iter 190563: 0.250618 and learning rate: 0.007120\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.7%\n",
      "Minibatch loss at epoch 123 and iter 192125: 0.295366 and learning rate: 0.007120\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.7%\n",
      "Minibatch loss at epoch 124 and iter 193687: 0.274457 and learning rate: 0.007120\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.7%\n",
      "Minibatch loss at epoch 125 and iter 195249: 0.273812 and learning rate: 0.006764\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.7%\n",
      "Minibatch loss at epoch 126 and iter 196811: 0.325890 and learning rate: 0.006764\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.7%\n",
      "Minibatch loss at epoch 127 and iter 198373: 0.314954 and learning rate: 0.006764\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.7%\n",
      "Minibatch loss at epoch 128 and iter 199935: 0.268431 and learning rate: 0.006764\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.7%\n",
      "Minibatch loss at epoch 129 and iter 201497: 0.274225 and learning rate: 0.006426\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 130 and iter 203059: 0.288879 and learning rate: 0.006426\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.7%\n",
      "Minibatch loss at epoch 131 and iter 204621: 0.311152 and learning rate: 0.006426\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.7%\n",
      "Minibatch loss at epoch 132 and iter 206183: 0.298846 and learning rate: 0.006104\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.7%\n",
      "Minibatch loss at epoch 133 and iter 207745: 0.242470 and learning rate: 0.006104\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.8%\n",
      "Minibatch loss at epoch 134 and iter 209307: 0.240914 and learning rate: 0.006104\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 135 and iter 210869: 0.310850 and learning rate: 0.005799\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.6%\n",
      "Minibatch loss at epoch 136 and iter 212431: 0.216116 and learning rate: 0.005799\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.7%\n",
      "Minibatch loss at epoch 137 and iter 213993: 0.348607 and learning rate: 0.005799\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.7%\n",
      "Minibatch loss at epoch 138 and iter 215555: 0.309271 and learning rate: 0.005509\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.8%\n",
      "Minibatch loss at epoch 139 and iter 217117: 0.240985 and learning rate: 0.005509\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.7%\n",
      "Minibatch loss at epoch 140 and iter 218679: 0.250479 and learning rate: 0.005509\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.8%\n",
      "Model saved\n",
      "Minibatch loss at epoch 141 and iter 220241: 0.281031 and learning rate: 0.005234\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.7%\n",
      "Minibatch loss at epoch 142 and iter 221803: 0.293748 and learning rate: 0.005234\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 143 and iter 223365: 0.360522 and learning rate: 0.005234\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 144 and iter 224927: 0.278392 and learning rate: 0.005234\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.8%\n",
      "Minibatch loss at epoch 145 and iter 226489: 0.303290 and learning rate: 0.004972\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 146 and iter 228051: 0.285441 and learning rate: 0.004972\n",
      "Minibatch  train and validation accuracy: 93.8%, 91.8%\n",
      "Minibatch loss at epoch 147 and iter 229613: 0.304455 and learning rate: 0.004972\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.7%\n",
      "Minibatch loss at epoch 148 and iter 231175: 0.343578 and learning rate: 0.004723\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 149 and iter 232737: 0.270030 and learning rate: 0.004723\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.7%\n",
      "Minibatch loss at epoch 150 and iter 234299: 0.224776 and learning rate: 0.004723\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 151 and iter 235861: 0.249804 and learning rate: 0.004487\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 152 and iter 237423: 0.212559 and learning rate: 0.004487\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 153 and iter 238985: 0.241244 and learning rate: 0.004487\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 154 and iter 240547: 0.220823 and learning rate: 0.004263\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.7%\n",
      "Minibatch loss at epoch 155 and iter 242109: 0.294393 and learning rate: 0.004263\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 156 and iter 243671: 0.348236 and learning rate: 0.004263\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.8%\n",
      "Minibatch loss at epoch 157 and iter 245233: 0.245569 and learning rate: 0.004050\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 158 and iter 246795: 0.171835 and learning rate: 0.004050\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.8%\n",
      "Minibatch loss at epoch 159 and iter 248357: 0.235876 and learning rate: 0.004050\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.9%\n",
      "Minibatch loss at epoch 160 and iter 249919: 0.274521 and learning rate: 0.004050\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 161 and iter 251481: 0.277023 and learning rate: 0.003847\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 162 and iter 253043: 0.217960 and learning rate: 0.003847\n",
      "Minibatch  train and validation accuracy: 94.5%, 91.8%\n",
      "Minibatch loss at epoch 163 and iter 254605: 0.233419 and learning rate: 0.003847\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.8%\n",
      "Minibatch loss at epoch 164 and iter 256167: 0.230435 and learning rate: 0.003655\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.8%\n",
      "Minibatch loss at epoch 165 and iter 257729: 0.269547 and learning rate: 0.003655\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 166 and iter 259291: 0.298097 and learning rate: 0.003655\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.8%\n",
      "Minibatch loss at epoch 167 and iter 260853: 0.224709 and learning rate: 0.003472\n",
      "Minibatch  train and validation accuracy: 95.3%, 91.8%\n",
      "Minibatch loss at epoch 168 and iter 262415: 0.223519 and learning rate: 0.003472\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 169 and iter 263977: 0.237269 and learning rate: 0.003472\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 170 and iter 265539: 0.266749 and learning rate: 0.003299\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 171 and iter 267101: 0.202693 and learning rate: 0.003299\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 172 and iter 268663: 0.195825 and learning rate: 0.003299\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 173 and iter 270225: 0.314887 and learning rate: 0.003134\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 174 and iter 271787: 0.254517 and learning rate: 0.003134\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 175 and iter 273349: 0.278784 and learning rate: 0.003134\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 176 and iter 274911: 0.324143 and learning rate: 0.003134\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 177 and iter 276473: 0.221379 and learning rate: 0.002977\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Model saved\n",
      "Minibatch loss at epoch 178 and iter 278035: 0.299721 and learning rate: 0.002977\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Model saved\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minibatch loss at epoch 179 and iter 279597: 0.255999 and learning rate: 0.002977\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 180 and iter 281159: 0.272674 and learning rate: 0.002828\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 181 and iter 282721: 0.220408 and learning rate: 0.002828\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 182 and iter 284283: 0.278762 and learning rate: 0.002828\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 183 and iter 285845: 0.218324 and learning rate: 0.002687\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 184 and iter 287407: 0.267228 and learning rate: 0.002687\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 185 and iter 288969: 0.225403 and learning rate: 0.002687\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 186 and iter 290531: 0.255125 and learning rate: 0.002552\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 187 and iter 292093: 0.260651 and learning rate: 0.002552\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 188 and iter 293655: 0.228662 and learning rate: 0.002552\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 189 and iter 295217: 0.237170 and learning rate: 0.002425\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 190 and iter 296779: 0.283473 and learning rate: 0.002425\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 191 and iter 298341: 0.249626 and learning rate: 0.002425\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 192 and iter 299903: 0.209033 and learning rate: 0.002425\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 193 and iter 301465: 0.184462 and learning rate: 0.002303\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 194 and iter 303027: 0.267947 and learning rate: 0.002303\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 195 and iter 304589: 0.245524 and learning rate: 0.002303\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 196 and iter 306151: 0.271102 and learning rate: 0.002188\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 197 and iter 307713: 0.213250 and learning rate: 0.002188\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 198 and iter 309275: 0.240878 and learning rate: 0.002188\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.9%\n",
      "Minibatch loss at epoch 199 and iter 310837: 0.292601 and learning rate: 0.002079\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Minibatch loss at epoch 200 and iter 312399: 0.221063 and learning rate: 0.002079\n",
      "Minibatch  train and validation accuracy: 96.1%, 91.8%\n",
      "Final Test accuracy: 96.4%\n",
      "Test accuracy with the best model: 96.3%\n",
      "Total run time 10.6288 minutes\n"
     ]
    }
   ],
   "source": [
    "n_epochs = 200 # number of epochs\n",
    "\n",
    "# early-stopping parameters\n",
    "patience = 5000 # look as this many examples regardless\n",
    "patience_increase = 2\n",
    "improvement_threshold = 0.995 # a relative improvement of \n",
    "                                  # this much is considered significant\n",
    "best_valid_loss = np.inf\n",
    "start_time = timeit.default_timer()\n",
    "\n",
    "n_train_batches = train_dataset.shape[0] // batch_size\n",
    "\n",
    "valid_freq = min(n_train_batches, patience // 2)\n",
    "\n",
    "with tf.Session(graph=g) as sess:\n",
    "    tf.global_variables_initializer().run()\n",
    "    print('Initialized')\n",
    "    done_looping = False\n",
    "    epoch = 0\n",
    "    while (epoch < n_epochs) and (not done_looping):\n",
    "        epoch = epoch + 1\n",
    "        for minibatch_index in range(n_train_batches):\n",
    "            \n",
    "            batch_data = \\\n",
    "                train_dataset[minibatch_index * batch_size:(minibatch_index + 1) * batch_size, :]\n",
    "            batch_labels = \\\n",
    "                train_labels[minibatch_index * batch_size:(minibatch_index + 1) * batch_size, :]\n",
    "            feed_dict = {tf_train_dataset : batch_data, tf_train_labels : batch_labels}\n",
    "            _, l, predictions = sess.run(\n",
    "                [optimizer, loss, train_prediction], feed_dict=feed_dict)\n",
    "        \n",
    "            iter = (epoch - 1) * n_train_batches + minibatch_index # cumulative iteration number\n",
    "        \n",
    "            if (iter + 1) % valid_freq == 0:\n",
    "                this_valid_loss = 1. - (accuracy(valid_prediction.eval(), valid_labels) / 100.)\n",
    "            \n",
    "                print(\"Minibatch loss at epoch %i and iter %i: %f and learning rate: %f\" % \n",
    "                      (epoch, iter, l, sess.run(learning_rate)))\n",
    "                print(\"Minibatch  train and validation accuracy: %.1f%%, %.1f%%\" \n",
    "                    % (accuracy(predictions, batch_labels), \n",
    "                    accuracy(valid_prediction.eval(), valid_labels)))\n",
    "            \n",
    "                if this_valid_loss < best_valid_loss:\n",
    "                    if this_valid_loss < best_valid_loss * improvement_threshold:\n",
    "                        patience = max(patience, iter * patience_increase)\n",
    "                    \n",
    "                    best_valid_loss = this_valid_loss\n",
    "                    \n",
    "                    params = (sess.run(W_0), sess.run(b_0),\n",
    "                              sess.run(W_1), sess.run(b_1),\n",
    "                              sess.run(W_2), sess.run(b_2))\n",
    "                    \n",
    "                    # save the best model\n",
    "                    with open('best_model_params.pkl', 'wb') as f:\n",
    "                            pickle.dump(params, f)\n",
    "                    print('Model saved')\n",
    "        \n",
    "            if patience <= iter:\n",
    "                    done_looping = True\n",
    "                    break\n",
    "                    \n",
    "    print(\"Final Test accuracy: %.1f%%\" \n",
    "                  % accuracy(test_prediction.eval(), test_labels))\n",
    "    \n",
    "    with open('best_model_params.pkl', 'rb') as f:\n",
    "        W_0_best, b_0_best, W_1_best, b_1_best, W_2_best, b_2_best = pickle.load(f)\n",
    "        \n",
    "    W_0_init, b_0_init = tf.assign(W_0, W_0_best), tf.assign(b_0, b_0_best)\n",
    "    W_1_init, b_1_init = tf.assign(W_1, W_1_best), tf.assign(b_1, b_1_best)\n",
    "    W_2_init, b_2_init = tf.assign(W_2, W_2_best), tf.assign(b_2, b_2_best)\n",
    "    \n",
    "    sess.run([W_0_init, b_0_init, W_1_init, b_1_init, W_2_init, b_2_init])\n",
    "    \n",
    "    print(\"Test accuracy with the best model: %.1f%%\" \n",
    "                  % accuracy(test_prediction.eval(), test_labels))\n",
    "\n",
    "    end_time = timeit.default_timer()\n",
    "\n",
    "    print('Total run time %.4f minutes' % ((end_time - start_time) / 60.))"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "default_view": {},
   "name": "3_regularization.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
